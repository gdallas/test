{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import collections\n",
    "import datetime, dateutil.parser\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#data downloaded from this query\n",
    "#https://query.wikidata.org/#PREFIX%20wikibase%3A%20%3Chttp%3A%2F%2Fwikiba.se%2Fontology%23%3E%0APREFIX%20wd%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fprop%2Fdirect%2F%3E%0APREFIX%20rdfs%3A%20%3Chttp%3A%2F%2Fwww.w3.org%2F2000%2F01%2Frdf-schema%23%3E%0A%0ASELECT%20DISTINCT%20%3Fhuman%20%3Fcause%20%3Fdate_of_birth%20%3Fdate_of_death%20%3Fcountry%20%3Fsex_or_gender%20%20%3Freligion%20%3Fchildren%20WHERE%20%7B%0A%20%20%3Fh%20wdt%3AP31%20wd%3AQ5.%0A%20%20%3Fh%20wdt%3AP1196%20%3Fcid.%0A%20%20%3Fh%20wdt%3AP569%20%3Fdate_of_birth.%0A%20%20%3Fh%20wdt%3AP570%20%3Fdate_of_death.%0A%20%20%3Fh%20wdt%3AP27%20%3Fcount%0A%20%20Filter%20%28%28str%28%3Fcause%29%29%20%3D%20%22suicide%22%20%7C%7C%20%28str%28%3Fcause%29%29%20%3D%22homicide%22%20%7C%7C%20%28str%28%3Fcause%29%29%20%3D%20%22natural%20causes%22%20%7C%7C%20%3Fcause%20%3D%20%22accident%22%29%0A%20%20OPTIONAL%20%7B%0A%20%20%20%20%3Fh%20rdfs%3Alabel%20%3Fhuman.%0A%20%20%20%20FILTER%28%28LANG%28%3Fhuman%29%29%20%3D%20%22en%22%29%0A%20%20%7D%0A%20%20OPTIONAL%20%7B%0A%20%20%20%20%3Fcid%20rdfs%3Alabel%20%3Fcause.%0A%20%20%20%20FILTER%28%28LANG%28%3Fcause%29%29%20%3D%20%22en%22%29%0A%20%20%7D%0A%20%20OPTIONAL%20%7B%0A%20%20%20%20%3Fcount%20rdfs%3Alabel%20%3Fcountry.%0A%20%20%20%20FILTER%28%28LANG%28%3Fcountry%29%29%20%3D%20%22en%22%29%0A%20%20%7D%0A%20%20%20%20OPTIONAL%20%7B%20%3Fh%20wdt%3AP21%20%3Fsex_or_gender.%20%7D%0A%0A%20%20%20%20OPTIONAL%20%7B%20%3Fh%20wdt%3AP140%20%3Freligion.%20%7D%0A%20%20%20%20OPTIONAL%20%7B%20%3Fh%20wdt%3AP1971%20%3Fchildren.%20%7D%0A%0A%7D%0A\n",
    "#39,342 entries\n",
    "#8 variable\n",
    "#['human', 'cause', 'date_of_birth', 'date_of_death', 'country', 'sex_or_gender', 'religion', 'children']\n",
    "#downloaded as a csv\n",
    "with open('wiki_download2.csv') as raw_data:\n",
    "    csv = csv.reader(raw_data, delimiter=',')\n",
    "    p = [a for a in csv] # load data\n",
    "    p.pop(0)\n",
    "    array = np.array(p)\n",
    "    #remove entries that are duplicated with different children (2842 names are duplicated, different religions mainly)\n",
    "    dups = [item for item, count in collections.Counter(array[:,0]).items() if count > 1] # duplicates\n",
    "    #only store the first instance of a name, delete the rest\n",
    "    print( len(dups))\n",
    "    print(p[0:100])\n",
    "    \n",
    "    array_no_dups = []\n",
    "    \n",
    "    for a in p:\n",
    "        \n",
    "        if a[0] not in [row[0] for row in array_no_dups]:\n",
    "            array_no_dups.append(a)\n",
    "    \n",
    "    \n",
    "    for a in array_no_dups:\n",
    "            a[2] = a[2][:4]\n",
    "            if a[2][0] == 't': #cleaning up some mess\n",
    "                \n",
    "                list1 = list(a[2])\n",
    "                list1[0] = '1'\n",
    "                a[2] = ''.join(list1)\n",
    "            a[3] = a[3][:4]\n",
    "            if a[3][0] == 't':\n",
    "                \n",
    "                list1 = list(a[3])\n",
    "                list1[0] = '1'\n",
    "                a[3] = ''.join(list1)\n",
    "            a.append(int(a[3])-int(a[2])) #add age as a column\n",
    "    \n",
    "    print(array_no_dups[:100])\n",
    "    \n",
    "    print(np.array(array_no_dups).shape)\n",
    "    \n",
    "    print(array.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    dups = [item for item, count in collections.Counter([row[0] for row in array_no_dups]).items() if count > 1] \n",
    "    print( len(dups))\n",
    "    print(dups)\n",
    "    \n",
    "    arr = np.array(array_no_dups)\n",
    "    \n",
    "    for a in arr:\n",
    "        if a[-2] == '':\n",
    "            a[-2] = 0\n",
    "    for a in arr:\n",
    "        if a[-3] == '':\n",
    "            a[-3] = 'Unknown' #populate fields with nothing in them\n",
    "    \n",
    "    \n",
    "    \n",
    "    arr.dump('full.txt')\n",
    "    \n",
    "    print(arr[0:100, :])\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one hot encode the categoricals that need encoding\n",
    "#country encoding\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "data = np.load('full.txt')\n",
    "print(data[:10,0])\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(data[:,4])\n",
    "\n",
    "print(list(le.classes_))\n",
    "\n",
    "b = le.transform(data[:,4]) \n",
    "\n",
    "b = b.reshape(-1,1)\n",
    "#for some reason there are no accidents in the data that I grabbed.... so there's only 3 categories of cause of death\n",
    "\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder() \n",
    "z = ohe.fit_transform(b).toarray()\n",
    "\n",
    "print(z.shape)\n",
    "print(data.shape)\n",
    "z.dump('countryencoding.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sex encoding\n",
    "\n",
    "#data = np.load('countryencoding.txt')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data[:,5])\n",
    "\n",
    "print(list(le.classes_))\n",
    "\n",
    "b = le.transform(data[:,5]) \n",
    "\n",
    "b = b.reshape(-1,1)\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder() \n",
    "z = ohe.fit_transform(b).toarray()\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "z.dump('sexencoding.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#religion encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data[:,6])\n",
    "\n",
    "print(list(le.classes_))\n",
    "\n",
    "b = le.transform(data[:,6]) \n",
    "\n",
    "b = b.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder() \n",
    "z = ohe.fit_transform(b).toarray()\n",
    "print(data.shape)\n",
    "print(z.shape)\n",
    "\n",
    "z.dump('religionencoding.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from random import shuffle\n",
    "data = np.load('full.txt')\n",
    "country = np.load('countryencoding.txt')\n",
    "sex = np.load('sexencoding.txt')\n",
    "religion = np.load('religionencoding.txt')\n",
    "p=[]\n",
    "print(data.shape[0])\n",
    "for x in range(0,data.shape[0]):\n",
    "    p.append([data[x,:], country[x,:], sex[x,:], religion[x,:]])\n",
    "#put one hot encoded values into input matrix\n",
    "\n",
    "for x in range(0,len(p)):\n",
    "    p[x] = [val for sublist in p[x] for val in sublist]\n",
    "    del p[x][6]\n",
    "    del p[x][5]\n",
    "    del p[x][4]\n",
    "    if p[x][1] == 'natural causes':\n",
    "        p[x][1] = '0'\n",
    "    elif p[x][1] == 'suicide':\n",
    "        p[x][1] = '1'\n",
    "        \n",
    "    elif p[x][1] == 'homicide':\n",
    "        p[x][1] = '2'\n",
    "    else:\n",
    "        print(p[x][1])\n",
    "#delete corrresponding strings to one-hot encodings, encode causes of death\n",
    "        \n",
    "print(len(p))\n",
    "print(p[14])\n",
    "print(sys.getsizeof(p))\n",
    "with open('fullypreprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(p, f)\n",
    "\n",
    "#c.dump('fullyencoded.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from random import shuffle\n",
    "\n",
    "f = pickle.load(open('fullypreprocessed.pkl', 'rb'))\n",
    "\n",
    "\n",
    "shuffle(f)\n",
    "#shuffle values, split training and test set\n",
    "test = f[0:3530][:]\n",
    "arr = f[3531:][:]\n",
    "\n",
    "print(len(f))\n",
    "print(len(test))\n",
    "print(len(arr))\n",
    "\n",
    "with open('testset.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)\n",
    "\n",
    "\n",
    "causes = []\n",
    "#get rid of causes, make that target value, delete name too\n",
    "for a in arr:\n",
    "    causes.append(a[1])\n",
    "    del a[1]\n",
    "    del a[0]\n",
    "print(arr[14])\n",
    "print(len(causes))\n",
    "print(len(arr))\n",
    "print(len(arr[1]))\n",
    "print(arr[13])\n",
    "print(causes[12])\n",
    "test_causes = []\n",
    "for a in test:\n",
    "    test_causes.append(a[1])\n",
    "    del a[1]\n",
    "    del a[0]    \n",
    "    \n",
    "print(test[13])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#train a decision tree\n",
    "clf = tree.DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='best')\n",
    "#print(arr[:100])\n",
    "clf = clf.fit(np.array(arr), np.array(causes))\n",
    "print(clf.score(np.array(test), np.array(test_causes)))\n",
    "pred = clf.predict(np.array(test))\n",
    "#print(pred[:1000])\n",
    "print('Accuracy % = ')\n",
    "print(100*metrics.accuracy_score(np.array(test_causes), pred)) # decision tree should have around 73% accuracy\n",
    "print(len(pred))\n",
    "r = 0\n",
    "for a in pred:\n",
    "    if a == '0':\n",
    "        r = r+1\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "#set up neural network, encode names and data types\n",
    "names = ['name', 'cause', 'dob', 'dod', 'country', 'sex', 'religion', 'kids', 'age']\n",
    "\n",
    "dtypes = {\n",
    "    'name': np.int32,\n",
    "    'cause': np.int32,\n",
    "    'dob': np.int32, \n",
    "    'dod': np.int32, \n",
    "    'country': str,\n",
    "    'sex': str,\n",
    "    'religion': str,\n",
    "    'kids':np.int32,\n",
    "    'age':np.int32\n",
    "    }\n",
    "data = np.load('full.txt')\n",
    "\n",
    "for p in data:\n",
    "    \n",
    "    if p[1] == 'natural causes':\n",
    "        p[1] = '0'\n",
    "    elif p[1] == 'suicide':\n",
    "        p[1] = '1'\n",
    "        \n",
    "    elif p[1] == 'homicide':\n",
    "        p[1] = '2'\n",
    "    else:\n",
    "        print(p)\n",
    "df = pd.DataFrame(data, columns=names)\n",
    "#encode causes, make pandas dataframe\n",
    "def voc_list(f, x):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    le.fit(f.values[:,x])\n",
    "\n",
    "    country_vocab = list(le.classes_)\n",
    "    return country_vocab\n",
    "\n",
    "c_vocab = voc_list(df, 4)\n",
    "s_vocab = voc_list(df, 5)\n",
    "r_vocab = voc_list(df, 6)\n",
    "#get corpus for country, religion and sex\n",
    "df['country'] = df['country'].str.encode('utf-8')\n",
    "df = df.drop('name', axis=1)\n",
    "df['dod'] = df['dod'].astype(int)\n",
    "df['dob'] = df['dob'].astype(int)\n",
    "df['kids'] = df['kids'].astype(int)\n",
    "df['age'] = df['age'].astype(int)\n",
    "df['cause'] = df['cause'].astype(int)\n",
    "df['country'] = df['country'].astype('S80') \n",
    "df['sex'] = df['sex'].astype('S80') \n",
    "df['religion'] = df['religion'].astype('S80') \n",
    "print(df.dtypes)\n",
    "print(df.iloc[0])\n",
    "print(df.iloc[15])\n",
    "#parse training and test set\n",
    "training_data = df[:3530]\n",
    "eval_data = df[3530:]\n",
    "training_label = training_data.pop('cause')\n",
    "eval_label =eval_data.pop('cause')\n",
    "\n",
    "print(training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialise input functions\n",
    "training_input_fn = tf.estimator.inputs.pandas_input_fn(x=training_data, y=training_label, batch_size=64, shuffle=True, num_epochs=None)\n",
    "eval_input_fn = tf.estimator.inputs.pandas_input_fn(x=eval_data, y=eval_label, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dob = tf.feature_column.numeric_column('dob')\n",
    "dod = tf.feature_column.numeric_column('dod')\n",
    "kids = tf.feature_column.numeric_column('kids')\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "country = tf.feature_column.categorical_column_with_vocabulary_list('country', vocabulary_list=c_vocab)\n",
    "sex = tf.feature_column.categorical_column_with_vocabulary_list('sex', vocabulary_list=s_vocab)\n",
    "religion = tf.feature_column.categorical_column_with_vocabulary_list('religion', vocabulary_list=r_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn_features = [\n",
    "    #numerical features\n",
    "    dob, dod, kids, age,\n",
    "    # densify categorical features:\n",
    "    tf.feature_column.indicator_column(country),\n",
    "    tf.feature_column.indicator_column(sex),\n",
    "    tf.feature_column.indicator_column(religion),\n",
    "  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3 layers, 10 neurons in each layer\n",
    "dnnregressor = tf.contrib.learn.DNNRegressor(feature_columns=dnn_features, hidden_units=[10, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnnregressor.fit(input_fn=training_input_fn, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score = dnnregressor.evaluate(input_fn=eval_input_fn)\n",
    "for x in accuracy_score:\n",
    "    print (x)\n",
    "    print(accuracy_score[x])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn import metrics\n",
    "pred = dnnregressor.predict_scores(input_fn=eval_input_fn)\n",
    "predictions = list(itertools.islice(pred, None))\n",
    "b=[]\n",
    "for a in predictions:\n",
    "    #print(a)\n",
    "    b.append(int(round(a)))\n",
    "    a = int(round(a))\n",
    "print('Accuracy % = ')\n",
    "print(100*metrics.accuracy_score(eval_label,b))\n",
    "\n",
    "#prediction score of 77% should come out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
